接上一篇 [切比雪夫不等式](https://segmentfault.com/a/1190000041656350)，本篇讨论统计学上一个非常重要的理论，即大数定律，它是概率论的基本理论。

大数定律的直观表达非常符合我们的直觉，例如一个普通硬币如果扔足够多次，那么正反面的次数将会无限接近于 50%；或者一个被做了弊的硬币，扔出正面的理论概率是 0.7，那么当我们扔足够多次时，正反面的次数将无限接近于 70% 和 30%。

这种从无数次重复实验逼近概率理论值的过程，就是大数定律所描述的事情：即当试验次数 $N$ 足够大时，事实频率（frequency）将会无限接近于理论概率（probability）。

作为一个正常思维的人看来这似乎是理所当然的，然而这是数学，这样一个看上去显而易见的结论却并不是公理，我们需要严格的理论证明。

### 辛钦大数定理

大数定律是几个定理的总称，我们这里讨论的是它的基础版本，也是所有其它后续定理的引理，即辛钦大数定理。

考虑一个随机变量 $X$，符合某种概率分布，它的期望值为 $E(X) = \mu$，方差为 $\sigma^2$；通常我们并不确切知道 $\mu$ 和 $\sigma^2$ 的真实值，只能用采样的方式来估计它们。每次采样一个 $X$ 的值，得到一连串的采样值：
$$
X_1, X_2, X_3 ... X_n
$$
它们是互相独立的，且都符合原始 $X$ 的分布。

辛钦大数定理阐述的是：当 $n$ 足够大时，这 $n$ 个采样数据的的平均值 $\overline X$ 将会无限接近于期望值 $\mu$。

然而这是一种直观表达，我们如何用严谨的数学语言来定义 “无限接近于期望值” 这件事情？这需要用到微积分中极限的相关概念。

对于任意 $\epsilon>0$，有：
$$
\lim\limits_{n\rightarrow+\infty}P(|\overline X - \mu| < \epsilon) = 1
$$
也就是说当 $n$ 趋向于 $+\infty$ 时，$\overline X$ 存在极限 $\mu$，这被称为 $\overline X$ 依概率收敛于 $\mu$。

有了严格的数学定义，我们再来思考如何证明这个看上去好像很显然的结论。

### 证明

由于 $X$ 的期望值为 $E(X) = \mu$，方差为 $\sigma^2$，现在我们来考虑 $\overline X$ 的期望值和方差。实际上我们有如下结论：
$$
E(\overline X) = E(X) = \mu
$$
即 $\overline X$ 的期望值等于原始 $X$ 的期望值。

并且由于 $X_1, X_2 ... X_n$ 都是独立同分布的，根据方差的有关理论，我们有：
$$
D(X_1 + X_2 + ... + X_n) = D(X_1) + D(X_1) + ... + D(X_n) = n\sigma^2 
$$
因此 $\overline X$ 的方差可以计算：
$$
\begin{align}
D(\overline X) & = D\,[{1\over n}(X_1 + X_2 + ... + X_n)]\\
& = {1 \over n^2}[D(X_1) + D(X_2) + ... + D(X_n)]\\
& = {1 \over n^2} \cdot n\sigma^2 = {\sigma^2 \over n}
\end{align}
$$
因此我们得到如下结论：
$$
E(\overline X) = \mu, \,\,\,\,D(\overline X) ={\sigma^2 \over n}
$$
注意这两个公式，不要当它们也是理所当然的，它们有着严格的前提条件，即 $X_1, X_2 ...X_n$ 是 **独立** 并且和 $X$ **同分布** 的；

-----------------

有了以上基础结论，我们得到一个很重要的结论，就是当我们取出足够多的采样数据 $X_i$ 时，它们的均值是和原始分布 $X$ 有着一样的期望值 $\mu$，然而方差却从 $\sigma^2$ 减小到了 $\sigma^2 \over n$；

从直观来说，就是当我们对采样数据取了平均以后，它的整体期望值是不变的，但是数据的方差减小了，整体的数据分布更集中了。

![](/home/hy/Desktop/Projects/math/statistics/imgs/big1.png)

有了 $\overline X$ 的期望值和方差，此时我们可以搬出切比雪夫不等式了，对于任意 $\epsilon>0$，有：
$$
P(|\overline X - \mu| \geq \epsilon) <= {D(\overline X) \over {\epsilon^2}} = {\sigma^2 \ \over {n \cdot \epsilon^2}}
$$
那么当 $n$ 趋向于无穷大时：
$$
\lim\limits_{n\rightarrow+\infty}P(|\overline X - \mu| \geq \epsilon) <= \lim\limits_{n\rightarrow+\infty}{\sigma^2 \ \over {n \cdot \epsilon^2}} = 0
$$
仔细体会这条式子，它究竟在表达什么？

切比雪夫不等式约束了距离 $\mu$ 太远的那部分数据的占比，它是由方差进行约束的；当 $n$ 足够大时，这个约束的上限就会无限接近于 0，所以 $\overline X$ 距离 $\mu$ 超过 $\epsilon$ 的部分的概率也就无限接近于 0;也就是说，不管 $\epsilon$ 多小，只要采样数量  $n$ 越来越大，所有的 $\overline X$ 都将越来越被约束在 $\mu$ 附近不超过 $\epsilon$ 的范围内 $[\mu - \epsilon, \mu + \epsilon]$，那么事实上我们就得到了 $\overline X$ 无限接近于 $\mu$ 。

从上面的图里我们也可以看出这一点，当 $n$ 越来越大，$\overline X$ 的方差就越来越小，整体的分布就越来越集中到了期望值 $\mu$ 附近；当 $n$ 趋向于无穷大时，方差接近于 0，整体的分布趋于一根集中在 $\mu$ 的竖线，这就表明此时的  $\overline X$ 已经无限接近于 $\mu$。